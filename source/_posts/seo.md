---
layout: post
title: "互联网常见名词解释"
date: 2017-12-18
tags: [html,seo]
comments: true
---

网站三要素：服务器、主机、空间

### 服务器知识

#### 服务器概念
放在机房提供公共性或者商业性服务的电脑就称之为服务器，服务器需要24小时运行。

#### 服务器分类
按照服务器的外观可以分为台式服务器和机架式服务器，机架式服务器包括普通机架式和刀片式服务器

#### 服务器系统概念
定义在服务器上的操作系统

#### 服务器系统分类

window 、Linux、Unix

#### 服务器对网站的重要性

1. 确保网站持续稳定：网站24小时都可以访问

    影响因素：硬件配置、操作系统、应用软件    

2. 确保网站速度够快：网站可以瞬间打开

    影响因素：避免流量大的机房、独立IP、带宽

### 主机知识

#### 定义

主机（也叫空间）是存放网站程序和数据的地方，是搭建网站的三要素之一。

#### 分类

按照地域分类：国内主机(较贵，需要备案，但是速度快)、国外主机(比较便宜，不需备案，速度较慢)

按照程序语言分类：PHP主机、ASP主机、全能主机

按操作系统分类：Linux主机、Windows主机

#### 其他参数
数据库大小、FTP、流量计算

我们搭建网站的时候，只需要购买相应配置的主机就行，不需要自己搭建服务器


#### 绑定域名

为了确保访问者访问你域名的时候会打开你存放在该空间的网页，在服务器上设置该域名有权限访问的过程叫做绑定域名

#### 域名解析

域名解析就是把域名解析到空间ip上，绑定域名就是在空间上绑定域名允许域名访问空间的内容。域名解析与域名绑定这两个工作都需要，才能让网站正常访问。

#### IP

在网络上有成千上万台主机，为了区分这些主机，人们给每台主机都分配了一个专门的地址，这个地址就是IP，IP的地址由4位数组成，每部分都不大于256，各部分之间由小数点分开，，每个ip地址都是唯一的。如：192.168.0.1

#### IP与域名
 
 由于要访问网络时，记IP地址比较麻烦，从而给IP转换了一种比较简单易记的名称：域名。由于ip是唯一的，所以域名也是唯一的。

 #### VPN
 在公用网络上建立专用网络的技术，允许特定的用户访问公用网络的内部资源，那么这个虚拟专用网络就是VPN。

 #### VPN作用
 VPN作用：传输数据安全可靠，连接方便灵活，完全控制可扩充性，专用网络

 #### 网络日志
 服务器日志的统称，它是记录web服务器接收处理请求以及运行时错误等各种原始信息的、以.log结尾的文件

日志内容：服务器错误信息+用户访问信息+蜘蛛爬行信息。

通过网络日志可以清楚的得知访客的详细的信息，比如IP、时间、操作系统、是否成功访问某页面等。

由于网络日志可以记录各搜索引擎蜘蛛机器人在网站爬行的详细情况，所以网站管理员可以通过日志了解网站在搜索引擎中的表现，从而根据日志分析结果来调整seo策略。

### 网站类别

#### 企业网站

企业网站就是企业在互联网上进行网络建设和形象宣传的电子商务平台

分为：
1. 电子商务类型：以网络销售为主要目的的企业网站类型

2. 多媒体广告型：用来展示企业形象，打造企业品牌

3. 产品展示型：针对定向用户展示企业产品详细参数，提供服务咨询的企业网站类型


#### 商城网站
商城就是以电子商务软件来构建的大型商品电子交易平台，其主要作用就是通过商城交易平台向客户准确、快捷的销售产品

商城、消费者和物流构成了电子商务的核心三要素，其中商城是实现电子商务活动的载体

商城模式：

1. B2B:Business-to-Business，代表：阿里巴巴 慧聪网
2. B2C:Business-to-Customer，代表：京东、当当、凡客
3. C2C:Customer-to-Customer，代表：淘宝


### 关键词

SEO真正的价值是带来定向客户，定向客户是通过搜索一些词语来到我们网站的，这些客户要搜索的词语就叫做关键词。简单的来说，关键词就是潜在用户要搜的词，所有我们潜在客户在搜素引擎上面要搜索的词我们都可以称之为关键词。分为主关键词与长尾关键词。一般我们用最高权重页面去优化的词叫做主关键词。长尾关键词：搜索量没有主关键词打的一些词，一般用内页来优化。

### 文章
文章是一个网站必备的因素，是我们优化网站必要的手段，因为想要让一个网页排有名且转化一个很重要的前提就是我们要提供有价值的内容。页面内容对于搜索引擎来说最重要的就是文章，好的文章可以增强页面在整个网络上的权威，相反不好甚至作弊的文章可能回调整个网站在搜索引擎上积累的信任度。

原创：在seo行业里原创文章就是搜索引擎之前未见过的文章。各大搜索引擎的数据库是独立的，如果谷歌收录的一篇文章百度引擎没有见过的话， 也算作原创文章。

伪原创文章：在一些搜索引擎已经抓取过得文章基础上经过加工编辑成的文章。比如将原本网络中已经存在的文章交换段落，替换关键词就是一篇伪原创文章。

### 链接

1. 反向链接：任何一个链接都是某一个页面对另一个页面的反向链接，任何一个超链接都是一个页面对另外一个页面的信任投票。
2. 锚文本：就是文本上加入超链接，如：[朽壳](https://sinsle.com/)，这是对另外一个页面最好的信任投票，直接的告诉了搜索引擎这个链接的属性
3. URL超链接：就是网址的超级链接，如：https://sinsle.com/

#### 内链
内链是反向链接的一种形式，主要是指我们网站内部网页之间的相互超链接。

作用：可以让搜索引擎更多的抓取我们的页面，以及可以让网页之间进行"投票"。内链具有相关、稳定的特点。而这两个特点在反向链接中非常重要。

注意：内部链接虽然非常重要，但是我们在刚开始做一个网站的时候不要加太多内部链接，因为有过度优化的嫌疑。

#### 外链

在同等条件下，外链数量较多，排名和稳定性越好。同等条件下，越是相关的页面给的外链越有价值。

外链是其他网站对本网站的信任投票，利于搜索引擎对我们网页权威性的评价。

注意：在建站初期时，建议大家不要建设太多外链。


### http状态码
我们在访问任何一个网页，服务器都会生成网站的日志，在日志里面会有一个3位数的数字代码，这三位数字diam就是记录了我们访问的动作，比如是“正常访问”还是“无法打开网页”等，这三位数字就是http状态码。

1xx:消息，代表请求已被接受，需要继续处理。这类响应是临时响应，只包含状态行和某些可选的响应头信息，并以空行结束。

2xx:成功，代表请求已成功被服务器接收、理解、并接受。

3xx:重定向，代表需要客户端采取进一步的操作才能完成请求。通常，这些状态码用来重定向，后续的请求地址（重定向目标）在本次响应的Location域中指明。

4xx:客户端错误，代表了客户端看起来可能发生了错误，妨碍了服务器的处理。

5xx:服务器错误，表示服务器无法完成明显有效的请求。[56]这类状态码代表了服务器在处理请求的过程中有错误或者异常状态发生，也有可能是服务器意识到以当前的软硬件资源无法完成对请求的处理。

http状态码是服务器与客户端交流信息的语言，这个客户端包括用户与爬虫。通过站长工具的http状态码查询可以查看网站的状态码信息

![ ](http://p09xm7bj0.bkt.clouddn.com/seo1.png)

通过http状态码可以查看搜索引擎爬虫在你网站的爬取情况，也可以查看网站出现了什么问题。

### 爬虫
网络爬虫(网络蜘蛛)是一种自动获取网页内容的程序，爬虫抓取的网页将会被搜索引擎系统存储，进行一定的分析、过滤并建立索引，一边之后的用户能够查询到这个页面，这个获取信息的程序就是爬虫。

爬虫为搜索引擎收集内容，搜索引擎展示的内容大部分是爬虫收集的。

爬虫通过漫游的形式进行抓取，爬虫爬到一个页面后，看到一个链接，然后顺着那个连接又爬到另外一个页面，爬虫是不听的从一个页面跳到另一个页面的，他一边下载这个网页，一边在提取这个网页中的链接，那个页面上所有的链接都放在一个公用的“待抓取列表”里。而且爬虫有个特点，就是在访问你网站之前不去做判断你这个网站本身是怎么样的，不对网页内容判断就抓取，但是会有优先级的划分，尽量不会抓重复的内容，尽量抓重要内容(比如网站的公共部分).搜索引擎同时会排出多个爬虫进行多线程的抓取。所有被爬虫抓取的网页将会被系统存储，进行一定的分析过滤，并建立索引，以便之后的查询和检索。

### 权重

概念：搜索引擎对一个网站的信任度。

决定一个网站权重的因素包含很多方面，据谷歌数据调查至少有两百多个方面，且权重没有特定的工具可以测量出来。

### PR(Page Rank:网页级别)

PR是Google用来标识网页的等级、重要性的一种方法，是Google用来衡量一个网站好坏的重要标准之一。

### robots

搜索引擎使用爬虫程序自动访问互联网上的网页并获取网页信息。爬虫在访问一个网站时，会首先检查该网站的根目录下是否有一个叫做robots.txt文件，这个文件是用于指定爬虫在网站上抓取范围。简单的来说就是网站通过robots协议来告诉搜索引擎哪些页面可以抓取，哪些页面不可以抓取。

因为一些系统中的URL是大小写敏感的，所以robots.txt的文件名应统一为小写。

用法：

1. 允许所有的机器人
   ```
    User-agent:*
    Disallow:

    User-agent:*
    Allow:/
   ```
2. 允许特定的机器人
  ```
    User-agent:some_spider
    Allow:
  ```
3. 仅禁止特定机器人访问
  ```
    User-agent:any_spider
    Disallow:/
  ```
4. 禁止所有的机器人
  ```
    User-agent:*
    Disallow:/
  ```
5. 禁止所有机器人访问特定目录
  ```
    User-agent: *
    Disallow: /cgi-bin/
    Disallow: /images/
    Disallow: /tmp/
    Disallow: /private/
  ```
6. 禁止所有机器人访问特定文件类型
  ```
    User-agent: *
    Disallow: /*.php$
    Disallow: /*.js$
    Disallow: /*.inc$
    Disallow: /*.css$
  ```
7. 使用"\*"来限制访问的url，如仅允许访问以".html"为后缀的url
  ```
    User-agent: *
    Disallow: /cgi-bin/*.htm
  ```
8. 禁止访问网站中所有的动态界面
  ```
    User-agent: *
    Disallow: /*?*
  ```
### 查询网站收入量

浏览器中输入：site:www.zlstory.com

从新站录入的时刻来讲，google快于百度，百度关于新站不会马上录入，乃至适当长的时刻内都不会录入，只需经过百度的检测期后，百度才会许多录入该站点内容，这需求很长一段时刻。

### 网页快照
搜索引擎在收录网页中，对网页进行备份时，存在自己的服务器缓存里，当用户在搜索引擎中点击“网页快照”链接时，搜索引擎将蜘蛛系统当时所抓取并保存的网页内容展现出来，称为“网页快照”。

快照回档：指的是百度的快照被退回到之前日期的快照。

回档原因：

1. 主机空间不稳定，网站首页的改动、作弊等
2. 服务器宕机

### 黑冒
#### 黑冒与白冒
黑冒：所有不符合引擎优化规范的优化技巧及方法

白冒：所有符合引擎优化规范的优化技巧及方法

#### 黑冒与白冒的区别
本质区别：是否站在了用户的角度去调整优化我们的网站，黑冒是纯粹站在搜索引擎的角度去做优化，几乎或者完全不考虑用户感受，甚至存在欺骗用户、欺骗搜素引擎的行为。

#### 黑冒优点
好的黑冒是白冒seo的基础，是我们探索搜索引擎底线的工具。

#### 黑冒常用手段
1.锚文本轰炸

一个页面并没有相关的内容，但是用大量的锚文本指向此页面。

2.采集

用一些程序在网络是哪个自动收集一些文字，经过简单的程序自动处理之后发布到网站上，用户体验极差。

3.群发

用软件把自己的链接发布到一些网站上，短时间获得大量的外链。

4.站群和链轮

为了快速得到排名，组织了相关站点，这些网站之间按照一定规则像车轮一样链接起来，并且每一个站点都指向我们要优化的网站，因为链轮往往涉及都的网页较多，只要其中一个网页获得搜索引擎的光顾，其他网页通过这个网页上的超链接增加其在搜索引擎面前的曝光量，对于站群和链轮来讲，网站的数量越多风险越大。

5.挂马

为了达到某种目的，通过一种手段，进入到一个网站并在该网站安装了木马程序，不但该网站被挂马，更重要的是该网站用户的电脑也有中毒的危险，导致用户体验极差。

6.黑链

不正当的链接，用户一般看不到，但是搜索引擎可以看到的链接，一般网站后台被入侵，挂上了对方网站的链接，这些链接虽然从页面中看不出来，但是搜索引擎是可以抓取的。

#### 惩罚
如果滥用黑冒过度并且影响到正常的搜索秩序的时候，搜索引擎必然会做出调整。

1. 收录大量减少
2. 排名全面下降
3. 直接从搜索引擎数据库中删除该站

(换个域名就解决)

### PPC
Pay Per Click：点击付费广告。

自然排名：不管任何关键词在百度搜索结果当中每页肯定会有十个网页是按照搜索引擎规则得出的顺序进行排列的，这些网站的排名就是自然排名。

而搜索引擎的PPC，就是给的钱越多则排名越靠前、排名靠前的时间也越长。

